{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d45bdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 91 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:03<00:00, 27.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 22227 patches.\n",
      "Successfully saved dataset to train_data.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Parameters ---\n",
    "HR_IMAGE_DIR = 'Data/T91' # Folder with your high-res training images (e.g., T91 dataset)\n",
    "H5_OUTPUT_FILE = 'train_data.h5'\n",
    "UPSCALE_FACTOR = 2\n",
    "PATCH_SIZE = 32  # The size of the HR patches to be extracted\n",
    "STRIDE = 14      # The step size to move across the image for patch extraction\n",
    "\n",
    "# --- Script ---\n",
    "def create_training_dataset(hr_image_dir, h5_output_file):\n",
    "    \"\"\"\n",
    "    Preprocesses a directory of HR images to create an HDF5 dataset of LR/HR patch pairs.\n",
    "    \"\"\"\n",
    "    hr_image_paths = glob.glob(os.path.join(hr_image_dir, '*.png'))\n",
    "    \n",
    "    lr_patches = []\n",
    "    hr_patches = []\n",
    "\n",
    "    print(f\"Processing {len(hr_image_paths)} images...\")\n",
    "    \n",
    "    for img_path in tqdm(hr_image_paths):\n",
    "        try:\n",
    "            # Open HR image and convert to Y channel\n",
    "            hr_img = Image.open(img_path).convert('RGB')\n",
    "            hr_img_ycbcr = hr_img.convert('YCbCr')\n",
    "            hr_img_y, _, _ = hr_img_ycbcr.split()\n",
    "            \n",
    "            width, height = hr_img_y.size\n",
    "\n",
    "            # Extract patches\n",
    "            for i in range(0, height - PATCH_SIZE + 1, STRIDE):\n",
    "                for j in range(0, width - PATCH_SIZE + 1, STRIDE):\n",
    "                    # Define the box for cropping the HR patch\n",
    "                    box = (j, i, j + PATCH_SIZE, i + PATCH_SIZE)\n",
    "                    hr_patch = hr_img_y.crop(box)\n",
    "                    \n",
    "                    # Create the LR patch\n",
    "                    # 1. Downscale\n",
    "                    lr_size = (PATCH_SIZE // UPSCALE_FACTOR, PATCH_SIZE // UPSCALE_FACTOR)\n",
    "                    lr_patch_downscaled = hr_patch.resize(lr_size, Image.BICUBIC)\n",
    "                    # 2. Upscale back to original patch size (this is the model input)\n",
    "                    lr_patch_upscaled = lr_patch_downscaled.resize(hr_patch.size, Image.BICUBIC)\n",
    "                    \n",
    "                    # Convert to numpy arrays and append\n",
    "                    hr_patches.append(np.array(hr_patch))\n",
    "                    lr_patches.append(np.array(lr_patch_upscaled))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not process {img_path}: {e}\")\n",
    "\n",
    "    print(f\"Generated {len(hr_patches)} patches.\")\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    lr_patches = np.array(lr_patches)\n",
    "    hr_patches = np.array(hr_patches)\n",
    "    \n",
    "    # Add channel dimension (required by PyTorch Conv2d)\n",
    "    # Shape becomes (num_patches, 1, height, width)\n",
    "    lr_patches = np.expand_dims(lr_patches, axis=1)\n",
    "    hr_patches = np.expand_dims(hr_patches, axis=1)\n",
    "\n",
    "    # Save to HDF5 file\n",
    "    with h5py.File(h5_output_file, 'w') as hf:\n",
    "        hf.create_dataset('lr_images', data=lr_patches, compression=\"gzip\", chunks=True)\n",
    "        hf.create_dataset('hr_images', data=hr_patches, compression=\"gzip\", chunks=True)\n",
    "        \n",
    "    print(f\"Successfully saved dataset to {h5_output_file}\")\n",
    "\n",
    "\n",
    "# --- Run the function ---\n",
    "if __name__ == '__main__':\n",
    "    # Make sure the HR_IMAGE_DIR exists\n",
    "    if not os.path.isdir(HR_IMAGE_DIR) or not os.listdir(HR_IMAGE_DIR):\n",
    "         print(f\"Error: Directory '{HR_IMAGE_DIR}' is empty or does not exist.\")\n",
    "         print(\"Please download a dataset like T91 and place the images there.\")\n",
    "    else:\n",
    "        create_training_dataset(HR_IMAGE_DIR, H5_OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f331507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        # Layer 1: Patch extraction and representation\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=9, padding=4)\n",
    "        # Layer 2: Non-linear mapping\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, padding=0)\n",
    "        # Layer 3: Reconstruction\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=5, padding=2)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9332d2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import h5py\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Assume you have created an HDF5 file named 'train_data.h5'\n",
    "# with datasets 'lr_images' and 'hr_images'\n",
    "\n",
    "class SRDataset(Dataset):\n",
    "    def __init__(self, h5_file_path):\n",
    "        super(SRDataset, self).__init__()\n",
    "        self.h5_file = h5py.File(h5_file_path, 'r')\n",
    "        self.inputs = self.h5_file['lr_images']\n",
    "        self.labels = self.h5_file['hr_images']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Convert to PyTorch tensors\n",
    "        input_tensor = torch.from_numpy(self.inputs[index]).float()\n",
    "        label_tensor = torch.from_numpy(self.labels[index]).float()\n",
    "        return input_tensor, label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6197586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/100]:  11%|█         | 150/1390 [02:04<14:59,  1.38it/s, loss=1.01e+3]"
     ]
    }
   ],
   "source": [
    "# Train model on loaded data\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm # For a nice progress bar\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# --- Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SRCNN().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- Data Loading ---\n",
    "# Replace with the actual path to your HDF5 file\n",
    "train_dataset = SRDataset(h5_file_path='train_data.h5')\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for data, target in loop:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{NUM_EPOCHS}]\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# --- Save the model ---\n",
    "torch.save(model.state_dict(), 'srcnn_model.pth')\n",
    "print(\"Training complete. Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b67de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# --- Load the trained model ---\n",
    "model = SRCNN().to(device)\n",
    "model.load_state_dict(torch.load('srcnn_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# --- Prepare the input image ---\n",
    "# 1. Open a low-resolution image\n",
    "lr_image_path = 'path/to/your/low_res_image.png'\n",
    "img = Image.open(lr_image_path).convert('YCbCr')\n",
    "y, cb, cr = img.split()\n",
    "\n",
    "# 2. Upscale using bicubic interpolation (this is the model's input)\n",
    "# Note: The original paper does this step before feeding to the network\n",
    "y_bicubic = y.resize( (y.width * 2, y.height * 2), Image.BICUBIC) # Example upscale factor of 2\n",
    "\n",
    "# 3. Convert the Y channel to a tensor\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "input_tensor = transform(y_bicubic).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "# --- Run the model ---\n",
    "with torch.no_grad():\n",
    "    output_tensor = model(input_tensor)\n",
    "\n",
    "# --- Post-process the output ---\n",
    "output_tensor = output_tensor.cpu().squeeze(0)\n",
    "output_img_data = output_tensor.numpy().clip(0, 1) * 255.0\n",
    "output_img = Image.fromarray(np.uint8(output_img_data[0]), mode='L')\n",
    "\n",
    "# --- Merge with original color channels (upscaled) ---\n",
    "cb_bicubic = cb.resize(output_img.size, Image.BICUBIC)\n",
    "cr_bicubic = cr.resize(output_img.size, Image.BICUBIC)\n",
    "\n",
    "final_img = Image.merge('YCbCr', [output_img, cb_bicubic, cr_bicubic]).convert('RGB')\n",
    "final_img.save('output_high_res_image.png')\n",
    "print(\"Super-resolution complete. Image saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
